# Audit de Performance & Scalabilit√© - Izzico

**Date** : 18 janvier 2026
**Version** : 0.3.1
**Objectif** : Analyser la capacit√© de l'application √† supporter 10 000 utilisateurs mensuels actifs (MAU)
**Stack** : Next.js 14 + Supabase (Free tier) + Vercel Pro

---

## üìä R√âSUM√â EX√âCUTIF

L'application Izzico peut actuellement g√©rer **500-1000 utilisateurs actifs** sans probl√®me majeur. Cependant, des goulots d'√©tranglement critiques appara√Ætront √† **2000-5000 MAU** sur le plan gratuit de Supabase.

### Capacit√© par Palier

| Utilisateurs | √âtat | Actions Requises |
|--------------|------|------------------|
| **0-500** | ‚úÖ Fonctionnel | Aucune |
| **500-2000** | ‚ö†Ô∏è Ralentissements | Optimisations code (Phase 1) |
| **2000-5000** | üî¥ Probl√®mes critiques | Connection pooling + optimisations avanc√©es (Phase 2) |
| **5000-10000** | üí• Pannes fr√©quentes | Migration Supabase Pro obligatoire (Phase 3) |
| **10000+** | üöÄ Scalable | Supabase Pro + CDN + optimisations compl√®tes |

### Limites Techniques - Supabase Free Tier

| Ressource | Limite Free | Impact Critique √† |
|-----------|-------------|-------------------|
| **Connexions DB simultan√©es** | 50 | 2000 users actifs |
| **Storage** | 1 GB | 3-6 mois (10k users) |
| **Bandwidth** | 2 GB/mois | 1000 users actifs |
| **Real-time connections** | ~500 | 500 users actifs |
| **RAM Database** | 1 GB | 5000 users |

---

## üî• PROBL√àMES CRITIQUES IDENTIFI√âS

### 1. N+1 Query Pattern - Messagerie üî¥ CRITIQUE

**Fichier** : `contexts/MessagesContext.tsx`

**Probl√®me** :
- Chaque chargement de conversations d√©clenche 1 requ√™te initiale + N requ√™tes par conversation
- 1 utilisateur avec 10 conversations = 20-30 requ√™tes DB
- 100 utilisateurs simultan√©s = 2000-3000 requ√™tes/seconde ‚Üí saturation DB

**Impact √† √©chelle** :
- **100 users** : Ralentissement perceptible (1-2 sec)
- **1000 users** : Database CPU √† 80-100%
- **5000+ users** : Timeouts et pannes fr√©quentes

**Solution existante** : Le hook `lib/hooks/use-messages.ts` r√©sout d√©j√† ce probl√®me avec 3 requ√™tes parall√®les au lieu de N s√©quentielles.

**Action** : Remplacer `MessagesContext` par `use-messages` hook (2 jours)

---

### 2. Real-Time Subscription Explosion üî¥ CRITIQUE

**Fichiers** :
- `contexts/MessagesContext.tsx:354-379`
- `contexts/NotificationContext.tsx:87-101`

**Probl√®me** :
- Chaque utilisateur actif maintient 2-3 WebSocket connections permanentes
- Limite Supabase Free : 500 connexions simultan√©es
- √Ä 500 users actifs ‚Üí 100% de la capacit√© utilis√©e
- √Ä 1000+ users ‚Üí 95% des utilisateurs ne peuvent plus se connecter au real-time

**Effet cascade** :
```
Nouveau message arrive
  ‚Üì
Broadcast √† 1000 clients abonn√©s
  ‚Üì
Chaque client recharge TOUTES ses conversations (3 requ√™tes √ó 1000 = 3000 requ√™tes)
  ‚Üì
Database sature, timeouts g√©n√©ralis√©s
```

**Solutions** :
1. **Court terme** : Debouncing (regrouper updates toutes les 500ms)
2. **Moyen terme** : Polling intelligent au lieu de WebSocket permanent
3. **Long terme** : Server-Sent Events (SSE) avec mise en cache

---

### 3. Index Manquants sur Tables Critiques üî¥ CRITIQUE

**Impact** : Requ√™tes 10-100√ó plus lentes √† partir de 10k utilisateurs

**Index manquants identifi√©s** :

```sql
-- Lookup conversations par utilisateur (utilis√© √† chaque ouverture messagerie)
CREATE INDEX idx_conversation_participants_user_id
  ON conversation_participants(user_id, conversation_id);

-- Matching algorithm (utilis√© √† chaque recherche de colocataires)
CREATE INDEX idx_user_profiles_user_id_completion
  ON user_profiles(user_id, profile_completion_score);

-- Affichage r√©actions aux messages
CREATE INDEX idx_message_reactions_emoji_message
  ON message_reactions(emoji, message_id);

-- Mark messages as read (trigger performance)
CREATE INDEX idx_messages_conversation_sender_read
  ON messages(conversation_id, sender_id, read_at, created_at);
```

**Action** : Appliquer migration SQL (1 jour)

---

### 4. API Matching Sans Limite Stricte üî¥ CRITIQUE

**Fichier** : `app/api/matching/matches/route.ts:32-45`

**Probl√®me** :
- Client peut demander 100 matchs avec statistiques compl√®tes
- Chaque match = 3-5 JOINs + 50 colonnes = 500 KB de r√©ponse
- 10 000 users √ó 10 requ√™tes/minute = **166 MB/sec de bandwidth**

**Solution** :
1. R√©duire limite √† 20 matchs max par requ√™te
2. Ajouter pagination obligatoire
3. Impl√©menter cache c√¥t√© serveur (TTL: 5 min)

---

### 5. Stockage Images Sans Compression üü† HAUTE

**Fichier** : `lib/services/storage-service.ts`

**Probl√®me** :
- Avatars upload√©s en 2 MB (devrait √™tre 100 KB)
- Photos propri√©t√©s en 5 MB (devrait √™tre 500 KB)

**Projection croissance** :
- 10 000 users √ó 1.5 MB avatar = **15 GB**
- 500 propri√©t√©s √ó 60 MB photos = **30 GB**
- **Total : 45 GB** (limite Free tier = 1 GB)
- Quota d√©pass√© en **3 mois**

**Solution** : Compression automatique avec `sharp` avant upload (r√©duction 80-90%)

---

### 6. Endpoints API Sans Rate Limiting üü† HAUTE

**Endpoints non prot√©g√©s** :
- `/api/matching/matches` - Peut √™tre spamm√©
- `/api/matching/generate` - CPU intensif
- `/api/assistant/chat` - Co√ªt LLM √©lev√©
- `/api/rooms/search-aesthetic` - OCR co√ªteux

**Vuln√©rabilit√©** :
- Utilisateur malveillant peut g√©n√©rer ‚Ç¨100 de co√ªts OCR/LLM en 1 minute
- DDoS accidentel (bug client qui boucle sur API)

**Solution** : Rate limiting global avec Upstash Redis

---

### 7. Re-renders Inutiles - Contextes React üü† MOYENNE

**Fichiers** :
- `contexts/MessagesContext.tsx` - Pas de `useMemo` sur liste conversations
- `contexts/NotificationContext.tsx` - Pas de `useMemo` sur tableau notifications

**Probl√®me** :
- Chaque nouveau message d√©clenche re-render de TOUTES les conversations
- 50 conversations √ó 1000 users = 50 000 re-renders inutiles
- Impact UX : interface qui "lag" pendant le scroll

**Solution** : `useMemo` + `React.memo` sur composants enfants

---

## üìà PROJECTION CO√õTS

### Co√ªts par Palier d'Utilisateurs

| M√©trique | Actuel | 1K Users | 5K Users | 10K Users |
|----------|--------|----------|----------|-----------|
| **Co√ªt mensuel total** | ‚Ç¨7 (gratuit) | ‚Ç¨7-15 | ‚Ç¨25-50 | ‚Ç¨100-200 |
| **Supabase** | ‚Ç¨0 | ‚Ç¨0 | ‚Ç¨25 (Pro) | ‚Ç¨25-100 |
| **Storage** | ‚Ç¨0 | ‚Ç¨0 | ‚Ç¨5 (50GB) | ‚Ç¨15 (100GB) |
| **Bandwidth** | ‚Ç¨0 | ‚Ç¨0 | ‚Ç¨10 | ‚Ç¨50 (1TB) |
| **OCR/LLM** | ‚Ç¨5 | ‚Ç¨10 | ‚Ç¨30 | ‚Ç¨60 |
| **Vercel Pro** | ‚Ç¨20/mois | ‚Ç¨20 | ‚Ç¨20 | ‚Ç¨20 |

### Seuils de Migration Supabase

| Indicateur | Valeur Critique | Action |
|------------|-----------------|--------|
| **Connexions DB** | >40/50 (80%) | Migrer vers Pro |
| **Storage** | >800 MB (80%) | Migrer vers Pro |
| **Bandwidth** | >1.6 GB/mois (80%) | Optimiser images + migrer |
| **Latence API** | >2 sec (p95) | Optimiser queries + pooling |
| **Real-time disconnects** | >10%/jour | Redesign architecture |

---

## üéØ PLAN D'AM√âLIORATION PROGRESSIF

### PHASE 1 : OPTIMISATIONS CRITIQUES (Avant Lancement)
**Dur√©e** : 1 semaine
**Co√ªt** : ‚Ç¨0
**Impact** : Capacit√© 500 ‚Üí 2000 users

#### 1.1 √âliminer N+1 Queries (2 jours) ‚ö° PRIORIT√â MAX

**Objectif** : R√©duire de 70% les requ√™tes DB sur messagerie

**Actions** :
1. Remplacer `MessagesContext.tsx` par `lib/hooks/use-messages.ts`
2. Tester avec 100 conversations simul√©es
3. Mesurer before/after avec Supabase Performance Monitor

**Fichiers modifi√©s** :
- `contexts/MessagesContext.tsx` ‚Üí d√©pr√©cier
- Tous les composants utilisant MessagesContext ‚Üí migrer vers hook

**Validation** :
```bash
# Test de charge : 100 users √ó 10 conversations
npm run test:load-messaging
```

---

#### 1.2 Ajouter Index Manquants (1 jour)

**Migration SQL** :

```sql
-- Migration 125: Add critical indexes for scalability
-- File: supabase/migrations/125_add_scalability_indexes.sql

-- Conversation participants lookup (used on every inbox load)
CREATE INDEX IF NOT EXISTS idx_conversation_participants_user_conversation
  ON public.conversation_participants(user_id, conversation_id);

-- User profiles for matching algorithm
CREATE INDEX IF NOT EXISTS idx_user_profiles_user_completion
  ON public.user_profiles(user_id, profile_completion_score);

-- Message reactions display
CREATE INDEX IF NOT EXISTS idx_message_reactions_emoji_message
  ON public.message_reactions(emoji, message_id);

-- Messages mark-as-read performance (for trigger)
CREATE INDEX IF NOT EXISTS idx_messages_conversation_sender_read
  ON public.messages(conversation_id, sender_id, read_at, created_at);

-- Typing indicators cleanup performance
CREATE INDEX IF NOT EXISTS idx_typing_indicators_conversation_user
  ON public.typing_indicators(conversation_id, user_id);

-- Notifications user lookup
CREATE INDEX IF NOT EXISTS idx_notifications_user_created
  ON public.notifications(user_id, created_at DESC);
```

**Validation** :
```sql
-- V√©rifier que les index sont utilis√©s
EXPLAIN ANALYZE
SELECT * FROM conversation_participants
WHERE user_id = 'xxx' AND conversation_id = 'yyy';
-- Doit montrer "Index Scan" et non "Seq Scan"
```

---

#### 1.3 Rate Limiting Global (1 jour)

**Cr√©er middleware** : `lib/middleware/rate-limit.ts`

```typescript
import { Ratelimit } from '@upstash/ratelimit';
import { Redis } from '@upstash/redis';
import { NextRequest, NextResponse } from 'next/server';

const redis = new Redis({
  url: process.env.UPSTASH_REDIS_REST_URL!,
  token: process.env.UPSTASH_REDIS_REST_TOKEN!,
});

// Rate limiters par type d'endpoint
const limiters = {
  matching: new Ratelimit({
    redis,
    limiter: Ratelimit.slidingWindow(20, '1 m'), // 20 req/min
    analytics: true,
  }),
  assistant: new Ratelimit({
    redis,
    limiter: Ratelimit.slidingWindow(10, '1 m'), // 10 req/min
    analytics: true,
  }),
  expensive: new Ratelimit({
    redis,
    limiter: Ratelimit.slidingWindow(5, '1 m'), // 5 req/min (OCR, etc.)
    analytics: true,
  }),
};

export async function rateLimitMiddleware(
  request: NextRequest,
  type: 'matching' | 'assistant' | 'expensive',
  userId: string
) {
  const limiter = limiters[type];
  const { success, limit, remaining, reset } = await limiter.limit(userId);

  if (!success) {
    return NextResponse.json(
      {
        error: 'Too many requests',
        limit,
        remaining,
        reset: new Date(reset).toISOString(),
      },
      {
        status: 429,
        headers: {
          'X-RateLimit-Limit': limit.toString(),
          'X-RateLimit-Remaining': remaining.toString(),
          'X-RateLimit-Reset': reset.toString(),
        },
      }
    );
  }

  return null; // Rate limit OK, continuer
}
```

**Appliquer sur endpoints** :
- `app/api/matching/matches/route.ts`
- `app/api/matching/generate/route.ts`
- `app/api/assistant/chat/route.ts`
- `app/api/rooms/search-aesthetic/route.ts`

---

#### 1.4 Compression Images Automatique (2 jours)

**Installer d√©pendances** :
```bash
npm install sharp
```

**Modifier** : `lib/services/storage-service.ts`

```typescript
import sharp from 'sharp';

export class StorageService {
  // ... code existant ...

  /**
   * Optimise une image avant upload
   * - Avatars: 512√ó512 WebP, qualit√© 85
   * - Properties: max 2048px width WebP, qualit√© 85
   * - Documents: pas de compression
   */
  private async optimizeImage(
    file: File,
    type: 'avatar' | 'property' | 'document'
  ): Promise<Buffer> {
    // Skip non-images
    if (!file.type.startsWith('image/')) {
      return Buffer.from(await file.arrayBuffer());
    }

    const buffer = Buffer.from(await file.arrayBuffer());

    switch (type) {
      case 'avatar':
        return sharp(buffer)
          .resize(512, 512, { fit: 'cover', position: 'center' })
          .webp({ quality: 85 })
          .toBuffer();

      case 'property':
        return sharp(buffer)
          .resize(2048, null, { fit: 'inside', withoutEnlargement: true })
          .webp({ quality: 85 })
          .toBuffer();

      default:
        return buffer;
    }
  }

  async uploadAvatar(file: File, userId: string): Promise<UploadResult> {
    const optimized = await this.optimizeImage(file, 'avatar');
    const blob = new Blob([optimized], { type: 'image/webp' });
    const optimizedFile = new File([blob], file.name.replace(/\.\w+$/, '.webp'), {
      type: 'image/webp',
    });

    return this.uploadFile(optimizedFile, 'profile-photos', userId);
  }

  async uploadPropertyImage(file: File, propertyId: string): Promise<UploadResult> {
    const optimized = await this.optimizeImage(file, 'property');
    const blob = new Blob([optimized], { type: 'image/webp' });
    const optimizedFile = new File([blob], file.name.replace(/\.\w+$/, '.webp'), {
      type: 'image/webp',
    });

    return this.uploadFile(optimizedFile, 'property-images', propertyId);
  }
}
```

**Impact attendu** :
- Avatars : 2 MB ‚Üí 100 KB (95% r√©duction)
- Photos propri√©t√©s : 5 MB ‚Üí 500 KB (90% r√©duction)
- Storage total : 45 GB ‚Üí 5 GB (89% r√©duction)

---

#### 1.5 Optimiser Re-renders React (1 jour)

**Modifier** : `contexts/MessagesContext.tsx`

```typescript
import { useMemo } from 'react';

export function MessagesProvider({ children }: { children: React.ReactNode }) {
  const [conversations, setConversations] = useState<Conversation[]>([]);

  // M√©moriser la liste pour √©viter re-renders
  const memoizedConversations = useMemo(() => conversations, [conversations]);

  // M√©moriser les callbacks
  const sendMessage = useCallback(async (conversationId, content) => {
    // ... logique existante
  }, []);

  const markAsRead = useCallback(async (conversationId) => {
    // ... logique existante
  }, []);

  const value = useMemo(
    () => ({
      conversations: memoizedConversations,
      sendMessage,
      markAsRead,
      // ... autres valeurs
    }),
    [memoizedConversations, sendMessage, markAsRead]
  );

  return (
    <MessagesContext.Provider value={value}>
      {children}
    </MessagesContext.Provider>
  );
}
```

**M√©moriser composants enfants** :

```typescript
// components/messages/ConversationItem.tsx
import { memo } from 'react';

export const ConversationItem = memo(({ conversation, onClick }: Props) => {
  // ... render
}, (prevProps, nextProps) => {
  // Ne re-render que si la conversation change
  return prevProps.conversation.id === nextProps.conversation.id &&
         prevProps.conversation.lastMessage?.id === nextProps.conversation.lastMessage?.id;
});
```

---

### PHASE 2 : SCALABILIT√â AVANC√âE (√Ä 2000 Users)
**Dur√©e** : 2 semaines
**Co√ªt** : ‚Ç¨0-25/mois
**Impact** : Capacit√© 2000 ‚Üí 5000 users

#### 2.1 Connection Pooling avec PgBouncer (2 jours)

**Option 1 : Supabase Managed (recommand√©)**

Dans Supabase Dashboard :
1. Aller dans Settings ‚Üí Database
2. Activer "Connection Pooler" (gratuit sur Free tier)
3. Copier connection string : `postgresql://postgres:[password]@[host]:6543/postgres?pgbouncer=true`

**Modifier** : `.env.local`
```bash
# Ancienne connection (directe, limite 50)
# DATABASE_URL=postgresql://postgres:[password]@[host]:5432/postgres

# Nouvelle connection (pooled, limite 500+)
DATABASE_URL=postgresql://postgres:[password]@[host]:6543/postgres?pgbouncer=true
```

**Impact** :
- Connexions effectives : 50 ‚Üí 500+
- Latence : +5ms par requ√™te (acceptable)
- Capacit√© : 2000 users ‚Üí 5000 users

---

#### 2.2 Redesign Real-Time Architecture (5 jours)

**Probl√®me actuel** : WebSocket permanent pour chaque utilisateur

**Solution 1 : Polling Intelligent (recommand√© pour Free tier)**

```typescript
// lib/hooks/use-messages-polling.ts
import { useEffect, useRef } from 'react';

export function useMessagesPolling(userId: string) {
  const intervalRef = useRef<NodeJS.Timeout>();
  const [lastUpdate, setLastUpdate] = useState<Date>(new Date());

  useEffect(() => {
    // Polling toutes les 5 secondes quand onglet actif
    let pollInterval = 5000;

    const poll = async () => {
      const { data } = await supabase
        .from('messages')
        .select('id, created_at')
        .gt('created_at', lastUpdate.toISOString())
        .limit(1)
        .single();

      if (data) {
        // Nouveau message d√©tect√© ‚Üí recharger conversations
        setLastUpdate(new Date(data.created_at));
        loadConversations();

        // R√©duire interval √† 2 sec pendant 30 sec (conversation active)
        pollInterval = 2000;
        setTimeout(() => { pollInterval = 5000; }, 30000);
      }
    };

    // Poll seulement si onglet visible
    const handleVisibilityChange = () => {
      if (document.hidden) {
        clearInterval(intervalRef.current);
      } else {
        intervalRef.current = setInterval(poll, pollInterval);
      }
    };

    document.addEventListener('visibilitychange', handleVisibilityChange);
    intervalRef.current = setInterval(poll, pollInterval);

    return () => {
      clearInterval(intervalRef.current);
      document.removeEventListener('visibilitychange', handleVisibilityChange);
    };
  }, [userId, lastUpdate]);
}
```

**Impact** :
- Connexions WebSocket : 1000 ‚Üí 0 (√©limin√©es)
- Requ√™tes DB : +0.2 req/sec par user actif (n√©gligeable)
- Latence messages : <5 sec (acceptable pour messagerie non-instantan√©e)

**Solution 2 : Server-Sent Events (SSE) avec Debounce**

```typescript
// app/api/messages/stream/route.ts
import { NextRequest } from 'next/server';

export async function GET(request: NextRequest) {
  const userId = request.headers.get('x-user-id');

  const encoder = new TextEncoder();
  const stream = new ReadableStream({
    async start(controller) {
      const channel = supabase
        .channel(`user:${userId}:messages`)
        .on('postgres_changes', {
          event: 'INSERT',
          schema: 'public',
          table: 'messages',
        }, (payload) => {
          // Debounce : attendre 500ms avant d'envoyer
          setTimeout(() => {
            controller.enqueue(encoder.encode(`data: ${JSON.stringify(payload)}\n\n`));
          }, 500);
        })
        .subscribe();

      // Cleanup
      request.signal.addEventListener('abort', () => {
        channel.unsubscribe();
        controller.close();
      });
    },
  });

  return new Response(stream, {
    headers: {
      'Content-Type': 'text/event-stream',
      'Cache-Control': 'no-cache',
      'Connection': 'keep-alive',
    },
  });
}
```

---

#### 2.3 Cache Layer avec Redis (3 jours)

**Installer** : Upstash Redis (Free tier : 10k requests/day)

```typescript
// lib/cache/redis-cache.ts
import { Redis } from '@upstash/redis';

const redis = new Redis({
  url: process.env.UPSTASH_REDIS_REST_URL!,
  token: process.env.UPSTASH_REDIS_REST_TOKEN!,
});

export async function getCached<T>(
  key: string,
  fetcher: () => Promise<T>,
  ttl: number = 3600 // 1 heure par d√©faut
): Promise<T> {
  // V√©rifier cache
  const cached = await redis.get<T>(key);
  if (cached) return cached;

  // Sinon, fetch et mettre en cache
  const data = await fetcher();
  await redis.set(key, data, { ex: ttl });
  return data;
}
```

**Cacher** :

```typescript
// Profils utilisateurs (TTL: 1h)
const userProfile = await getCached(
  `user:${userId}:profile`,
  () => supabase.from('user_profiles').select('*').eq('user_id', userId).single(),
  3600
);

// Scores de matching (TTL: 24h)
const matchScores = await getCached(
  `user:${userId}:matches`,
  () => supabase.rpc('calculate_match_score', { p_user_id: userId }),
  86400
);

// Nombre de notifications (TTL: 5 min)
const notifCount = await getCached(
  `user:${userId}:notif_count`,
  () => supabase.from('notifications').select('id', { count: 'exact', head: true }),
  300
);
```

**Impact** :
- Requ√™tes DB : -30-50%
- Latence API : -100-300ms
- Co√ªt Upstash : ‚Ç¨0 (Free tier suffit)

---

#### 2.4 Pagination Stricte API Matching (1 jour)

**Modifier** : `app/api/matching/matches/route.ts`

```typescript
const querySchema = z.object({
  limit: z.coerce.number().int().min(1).max(20).default(10), // R√©duit de 100 ‚Üí 20
  offset: z.coerce.number().int().min(0).default(0),
  minScore: z.coerce.number().int().min(0).max(100).default(60),
  status: z.string().default('active').transform(s => s.split(',')),
  includeStats: z.coerce.boolean().default(false),
});

// Ajouter cache
const cacheKey = `matches:${userId}:${limit}:${offset}:${minScore}`;
const cached = await redis.get(cacheKey);
if (cached) {
  return NextResponse.json(cached, {
    headers: { 'X-Cache': 'HIT' },
  });
}

// ... requ√™te DB ...

// Mettre en cache 5 min
await redis.set(cacheKey, result, { ex: 300 });

return NextResponse.json(result, {
  headers: {
    'X-Cache': 'MISS',
    'Cache-Control': 'public, max-age=300',
  },
});
```

---

### PHASE 3 : MIGRATION SUPABASE PRO (√Ä 5000 Users)
**Dur√©e** : 1 semaine
**Co√ªt** : ‚Ç¨25-100/mois
**Impact** : Capacit√© 5000 ‚Üí 20 000+ users

#### 3.1 Quand Migrer ? Indicateurs Critiques

**Dashboard de Monitoring** : `lib/monitoring/supabase-metrics.ts`

```typescript
export async function checkMigrationThresholds() {
  const metrics = {
    connections: await getActiveConnections(),
    storage: await getStorageUsage(),
    bandwidth: await getBandwidthUsage(),
    apiLatency: await getApiLatencyP95(),
    realtimeDisconnects: await getRealtimeDisconnectRate(),
  };

  const thresholds = {
    connections: { critical: 40, max: 50 },
    storage: { critical: 800_000_000, max: 1_000_000_000 }, // 800 MB
    bandwidth: { critical: 1_600_000_000, max: 2_000_000_000 }, // 1.6 GB
    apiLatency: { critical: 2000, max: 5000 }, // 2 sec
    realtimeDisconnects: { critical: 0.1, max: 0.2 }, // 10%
  };

  const alerts = [];

  Object.keys(metrics).forEach((key) => {
    const value = metrics[key];
    const threshold = thresholds[key];

    if (value >= threshold.max) {
      alerts.push({
        severity: 'CRITICAL',
        metric: key,
        value,
        message: `${key} a atteint la limite (${value}/${threshold.max}). MIGRATION IMM√âDIATE REQUISE.`,
      });
    } else if (value >= threshold.critical) {
      alerts.push({
        severity: 'WARNING',
        metric: key,
        value,
        message: `${key} approche de la limite (${value}/${threshold.max}). Planifier migration.`,
      });
    }
  });

  return alerts;
}
```

**Automatiser check** : Vercel Cron Job quotidien

```typescript
// app/api/cron/check-metrics/route.ts
export async function GET(request: NextRequest) {
  const alerts = await checkMigrationThresholds();

  if (alerts.some(a => a.severity === 'CRITICAL')) {
    // Envoyer email/Slack notification
    await sendAlert({
      title: 'üö® MIGRATION SUPABASE PRO REQUISE',
      alerts,
    });
  }

  return NextResponse.json({ alerts });
}
```

---

#### 3.2 Plan de Migration Supabase Pro

**√âtapes** :

1. **Backup complet** (via Supabase Dashboard)
2. **Upgrade vers Pro** : ‚Ç¨25/mois
3. **Activer features** :
   - Database : 2 CPU ‚Üí 4 CPU, 1 GB RAM ‚Üí 4 GB RAM
   - Connexions : 50 ‚Üí 200
   - Storage : 1 GB ‚Üí 100 GB
   - Bandwidth : 2 GB ‚Üí 250 GB/mois
4. **Tester migration** : Cloner projet, upgrade, valider
5. **Basculer production** : Changer env vars Vercel

**Co√ªts additionnels estim√©s** :

| Service | Co√ªt Mensuel |
|---------|--------------|
| Supabase Pro | ‚Ç¨25 |
| Storage (100 GB) | ‚Ç¨15 |
| Bandwidth (1 TB) | ‚Ç¨50 |
| **Total** | **‚Ç¨90/mois** |

---

#### 3.3 Optimisations Post-Migration

**Archivage Messages** :

```sql
-- Migration 126: Archive old messages
-- D√©placer conversations inactives >90 jours vers table archive

CREATE TABLE IF NOT EXISTS public.messages_archive (
  LIKE public.messages INCLUDING ALL
);

-- Fonction d'archivage (√† ex√©cuter mensuellement)
CREATE OR REPLACE FUNCTION archive_old_messages()
RETURNS void AS $$
BEGIN
  WITH old_conversations AS (
    SELECT DISTINCT conversation_id
    FROM public.messages
    WHERE created_at < NOW() - INTERVAL '90 days'
    GROUP BY conversation_id
    HAVING MAX(created_at) < NOW() - INTERVAL '90 days'
  )
  INSERT INTO public.messages_archive
  SELECT m.*
  FROM public.messages m
  INNER JOIN old_conversations oc ON m.conversation_id = oc.conversation_id;

  -- Supprimer de la table principale
  DELETE FROM public.messages
  WHERE conversation_id IN (SELECT conversation_id FROM old_conversations);
END;
$$ LANGUAGE plpgsql SECURITY DEFINER;
```

**Optimiser RPC Functions** :

```sql
-- Ajouter caching dans RPC get_user_conversations
CREATE OR REPLACE FUNCTION get_user_conversations(p_user_id UUID)
RETURNS TABLE(...) AS $$
DECLARE
  v_cache_key TEXT := 'conversations:' || p_user_id::TEXT;
  v_cached JSONB;
BEGIN
  -- V√©rifier cache (via pg_advisory_lock ou extension)
  -- SELECT cached_value INTO v_cached FROM cache WHERE key = v_cache_key;

  -- Si cache valide, retourner
  -- IF v_cached IS NOT NULL THEN
  --   RETURN QUERY SELECT * FROM jsonb_to_recordset(v_cached) AS ...;
  --   RETURN;
  -- END IF;

  -- Sinon, requ√™te normale + mise en cache
  RETURN QUERY
  SELECT ... FROM conversation_participants WHERE user_id = p_user_id;
END;
$$ LANGUAGE plpgsql SECURITY DEFINER;
```

---

## üìä SYST√àME DE MONITORING

### Dashboard Performance √† Cr√©er

**Fichier** : `app/admin/performance/page.tsx`

```typescript
'use client';

import { useEffect, useState } from 'react';

export default function PerformanceDashboard() {
  const [metrics, setMetrics] = useState(null);

  useEffect(() => {
    fetch('/api/monitoring/metrics')
      .then(res => res.json())
      .then(setMetrics);
  }, []);

  return (
    <div className="p-8">
      <h1 className="text-3xl font-heading mb-8">Performance & Scalabilit√©</h1>

      {/* Connexions DB */}
      <MetricCard
        title="Connexions Database"
        value={metrics?.connections}
        max={50}
        critical={40}
        unit="connexions"
      />

      {/* Storage */}
      <MetricCard
        title="Storage Utilis√©"
        value={metrics?.storage}
        max={1_000_000_000}
        critical={800_000_000}
        unit="bytes"
        format={(v) => `${(v / 1_000_000_000).toFixed(2)} GB`}
      />

      {/* Bandwidth */}
      <MetricCard
        title="Bandwidth Mensuel"
        value={metrics?.bandwidth}
        max={2_000_000_000}
        critical={1_600_000_000}
        unit="bytes"
        format={(v) => `${(v / 1_000_000_000).toFixed(2)} GB`}
      />

      {/* Latence API */}
      <MetricCard
        title="Latence API (p95)"
        value={metrics?.apiLatency}
        max={5000}
        critical={2000}
        unit="ms"
      />

      {/* Real-time */}
      <MetricCard
        title="Taux D√©connexion Real-time"
        value={metrics?.realtimeDisconnects}
        max={0.2}
        critical={0.1}
        unit="%"
        format={(v) => `${(v * 100).toFixed(1)}%`}
      />

      {/* Recommandation migration */}
      {metrics?.shouldMigrate && (
        <div className="bg-red-50 border-2 border-red-500 rounded-xl p-6 mt-8">
          <h2 className="text-xl font-heading text-red-700 mb-4">
            üö® Migration Supabase Pro Recommand√©e
          </h2>
          <p className="text-red-600 mb-4">
            Un ou plusieurs indicateurs ont d√©pass√© le seuil critique.
          </p>
          <ul className="list-disc pl-6 text-red-600">
            {metrics.alerts.map((alert, i) => (
              <li key={i}>{alert.message}</li>
            ))}
          </ul>
        </div>
      )}
    </div>
  );
}

function MetricCard({ title, value, max, critical, unit, format }) {
  const percentage = (value / max) * 100;
  const isCritical = value >= critical;
  const isMax = value >= max;

  const color = isMax ? 'red' : isCritical ? 'orange' : 'green';

  return (
    <div className={`bg-white border-2 border-${color}-500 rounded-xl p-6 mb-4`}>
      <h3 className="text-lg font-heading mb-2">{title}</h3>
      <div className="flex items-end gap-4">
        <span className="text-3xl font-bold">
          {format ? format(value) : `${value} ${unit}`}
        </span>
        <span className="text-gray-500">/ {format ? format(max) : `${max} ${unit}`}</span>
      </div>
      <div className="w-full bg-gray-200 rounded-full h-4 mt-2">
        <div
          className={`bg-${color}-500 h-4 rounded-full transition-all`}
          style={{ width: `${Math.min(percentage, 100)}%` }}
        />
      </div>
      <p className="text-sm text-gray-600 mt-2">
        {percentage.toFixed(1)}% de la capacit√© utilis√©e
      </p>
    </div>
  );
}
```

---

## üß™ TESTS DE CHARGE

### Script Artillery (Load Testing)

**Fichier** : `tests/load/messaging.yml`

```yaml
config:
  target: "https://izzico.vercel.app"
  phases:
    # Mont√©e progressive
    - duration: 60
      arrivalRate: 5 # 5 users/sec
      name: "Warm up"
    - duration: 120
      arrivalRate: 20 # 20 users/sec = 2400 users
      name: "Ramp up"
    - duration: 300
      arrivalRate: 50 # 50 users/sec = 15000 users
      name: "Sustained load"
  environments:
    production:
      target: "https://izzico.vercel.app"
    staging:
      target: "https://staging-izzico.vercel.app"

scenarios:
  - name: "User Login + Load Conversations"
    flow:
      - post:
          url: "/api/auth/login"
          json:
            email: "test{{ $randomNumber() }}@example.com"
            password: "TestPassword123!"
          capture:
            - json: "$.token"
              as: "authToken"

      - get:
          url: "/api/messages/conversations"
          headers:
            Authorization: "Bearer {{ authToken }}"
          expect:
            - statusCode: 200

      - think: 5 # Pause 5 sec (utilisateur lit)

      - get:
          url: "/api/matching/matches?limit=10"
          headers:
            Authorization: "Bearer {{ authToken }}"
          expect:
            - statusCode: 200

      - think: 10

  - name: "Send Message"
    flow:
      - post:
          url: "/api/messages/send"
          headers:
            Authorization: "Bearer {{ authToken }}"
          json:
            conversationId: "{{ conversationId }}"
            content: "Test message {{ $randomString() }}"
          expect:
            - statusCode: 201
```

**Lancer tests** :

```bash
# Installer Artillery
npm install -g artillery

# Test de charge
artillery run tests/load/messaging.yml --output report.json

# G√©n√©rer rapport HTML
artillery report report.json --output report.html
```

**M√©triques √† surveiller** :

| M√©trique | Objectif | Critique |
|----------|----------|----------|
| Latence p50 | <500ms | >2000ms |
| Latence p95 | <1500ms | >5000ms |
| Latence p99 | <3000ms | >10000ms |
| Taux erreur | <1% | >5% |
| Requ√™tes/sec | 100+ | <10 |

---

## üìã CHECKLIST DE LANCEMENT

### Avant 500 Users

- [ ] Phase 1.1 : √âliminer N+1 queries (MessagesContext ‚Üí use-messages)
- [ ] Phase 1.2 : Ajouter index manquants (migration 125)
- [ ] Phase 1.3 : Rate limiting global (matching, assistant, OCR)
- [ ] Phase 1.4 : Compression images automatique (sharp)
- [ ] Phase 1.5 : Optimiser re-renders React (useMemo, React.memo)
- [ ] Test de charge : 100 users simultan√©s (Artillery)
- [ ] Dashboard monitoring : `/admin/performance`

### Avant 2000 Users

- [ ] Phase 2.1 : Connection pooling PgBouncer
- [ ] Phase 2.2 : Redesign real-time (polling ou SSE)
- [ ] Phase 2.3 : Cache layer Redis (profils, matchs, notifs)
- [ ] Phase 2.4 : Pagination stricte API matching (max 20)
- [ ] Test de charge : 500 users simultan√©s
- [ ] Monitoring quotidien : check seuils migration

### Avant 5000 Users

- [ ] √âvaluation migration Supabase Pro (seuils atteints ?)
- [ ] Backup complet database
- [ ] Test migration sur projet clone
- [ ] Migration Supabase Pro (si n√©cessaire)
- [ ] Phase 3.3 : Archivage messages anciens
- [ ] Phase 3.3 : Optimiser RPC functions avec cache
- [ ] Test de charge : 1000 users simultan√©s

### Avant 10000 Users

- [ ] Supabase Pro actif (obligatoire)
- [ ] CDN pour assets statiques (Cloudflare, Vercel CDN)
- [ ] Database scaling horizontal (read replicas)
- [ ] Monitoring avanc√© (Datadog, New Relic)
- [ ] SLA & Alerting 24/7

---

## üéØ R√âSUM√â PAR PRIORIT√â

### ‚ö° URGENT (Avant Lancement)

1. **√âliminer N+1 queries** - 2 jours - Impact √©norme
2. **Ajouter index DB** - 1 jour - Performance √ó10
3. **Compression images** - 2 jours - √âconomie 80% storage
4. **Rate limiting** - 1 jour - S√©curit√© DDoS

### üî• HAUTE (Avant 2000 Users)

5. **Connection pooling** - 2 jours - Capacit√© √ó5
6. **Redesign real-time** - 5 jours - √âliminer goulot WebSocket
7. **Cache Redis** - 3 jours - Latence -50%

### üü† MOYENNE (Avant 5000 Users)

8. **Migration Supabase Pro** - 1 semaine - D√©bloquer croissance
9. **Archivage messages** - 2 jours - Database plus l√©g√®re
10. **Optimiser RPC** - 3 jours - Performance +30%

---

## üìû CONTACT & SUPPORT

**Responsable Technique** : Samuel Baudon
**Date Audit** : 18 janvier 2026
**Prochaine R√©vision** : Apr√®s migration 2000 users

---

*Document g√©n√©r√© par audit automatis√© - Version 1.0*
